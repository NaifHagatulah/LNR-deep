# Quick Reference: Binary LNR Algorithm

## Algorithm at a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Binary Label Noise Rebalancing (LNR)                       â”‚
â”‚  For Imbalanced Binary Classification                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT:
  X: Features
  Y: Labels (0=majority, 1=minority)
  Cf: Pre-trained classifier
  t_flip: Threshold (default=3.0)

ALGORITHM:
  1. Get majority indices: IndMA â† {i : Y[i] = 0}
  
  2. Get predictions: Î·Ì‚[i] â† P(Y=1|X[i]) for all i
  
  3. Compute statistics on majority class:
     Î¼ â† mean(Î·Ì‚[IndMA])
     Ïƒ â† std(Î·Ì‚[IndMA])
  
  4. For each iMA in IndMA:
     a) Z[iMA] â† (Î·Ì‚[iMA] - Î¼) / Ïƒ         # Z-score
     b) Ï[iMA] â† max(tanh(Z - t_flip), 0)  # Flip rate
     c) U ~ Bernoulli(Ï[iMA])              # Sample
     d) if U = 1: Y[iMA] â† 1               # Flip

OUTPUT:
  Y: Modified labels (some 0â†’1 flips)
```

---

## Files Quick Reference

| File | Purpose | Run Command |
|------|---------|-------------|
| `demo_binary_lnr.py` | See algorithm in action | `python demo_binary_lnr.py` |
| `train_binary_lnr.py` | Train on CIFAR | `python train_binary_lnr.py --stage 1 ...` |
| `binary_lnr.py` | Core implementation | Import as module |
| `BINARY_README.md` | Complete documentation | Read for details |
| `REPO_ANALYSIS.md` | Understand repo | Read for architecture |

---

## Quick Start Commands

### Run Demo (No Setup Needed)
```bash
python demo_binary_lnr.py
```

### Train Stage 1 (Pre-train)
```bash
python train_binary_lnr.py \
  --stage 1 \
  --class0 0 --class1 1 \
  --imbalance 0.1 \
  --epochs 100 \
  --save-dir ./saved_binary/exp1
```

### Train Stage 2 (Apply LNR)
```bash
python train_binary_lnr.py \
  --stage 2 \
  --resume ./saved_binary/exp1/stage1_best.pth \
  --threshold 3.0 \
  --epochs 100 \
  --save-dir ./saved_binary/exp1
```

---

## Parameter Guide

### threshold (t_flip)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Value       â”‚ Flip Amount  â”‚ Use Case    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1.5 - 2.5   â”‚ Aggressive   â”‚ Very severe â”‚
â”‚             â”‚ (3-5% flips) â”‚ imbalance   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2.5 - 3.5   â”‚ Moderate     â”‚ Typical     â”‚
â”‚             â”‚ (1-3% flips) â”‚ imbalance   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4.0 - 5.0   â”‚ Conservative â”‚ Mild        â”‚
â”‚             â”‚ (<1% flips)  â”‚ imbalance   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Default: 3.0** (good starting point)

---

## Understanding Z-scores

```
Z = (prediction - mean) / std

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Z-score Interpretation                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Z < 0     : Below average (typical majority)   â”‚
â”‚ Z = 0     : Average majority sample            â”‚
â”‚ 0 < Z < 3 : Above average (confused sample)    â”‚
â”‚ Z > 3     : High outlier (flip candidate!)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flip Rate: Ï = max(tanh(Z - t_flip), 0)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ If t_flip = 3.0:                               â”‚
â”‚   Z = 2.0 â†’ Ï = 0.00 (no flip)                 â”‚
â”‚   Z = 3.0 â†’ Ï = 0.00 (threshold)               â”‚
â”‚   Z = 4.0 â†’ Ï = 0.76 (likely flip)             â”‚
â”‚   Z = 5.0 â†’ Ï = 0.96 (very likely flip)        â”‚
â”‚   Z = 10  â†’ Ï â‰ˆ 1.0  (almost certain flip)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Expected Performance

```
Example: 10% Minority Class (Severe Imbalance)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric          â”‚ Before   â”‚ After    â”‚ Change  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Overall Acc     â”‚ 92%      â”‚ 90%      â”‚ -2%     â”‚
â”‚ Majority Acc    â”‚ 99%      â”‚ 95%      â”‚ -4%     â”‚
â”‚ Minority Acc    â”‚ 40%      â”‚ 85%      â”‚ +45%    â”‚
â”‚ Balanced Acc    â”‚ 70%      â”‚ 90%      â”‚ +20%    â”‚
â”‚ F1 Score        â”‚ 0.55     â”‚ 0.88     â”‚ +0.33   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Trade-off: Small sacrifice in majority for large
           gain in minority â†’ Better balance!
```

---

## Code Snippet - Basic Usage

```python
from binary_lnr import BinaryLNR, apply_label_flips

# 1. Create LNR object
lnr = BinaryLNR(
    model=feature_extractor,
    classifier=classification_head,
    threshold=3.0,
    device='cuda'
)

# 2. Generate noise model (once at start)
noise_info = lnr.generate_noise_model(
    dataloader=train_loader,
    save_path='noise_model.pkl'
)

# 3. Training loop
for epoch in range(epochs):
    for indices, images, targets in train_loader:
        # Apply label flips
        targets = apply_label_flips(
            targets, indices, noise_info
        )
        
        # Train with flipped labels
        loss = train_step(images, targets)
```

---

## Troubleshooting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Problem              â”‚ Solution               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ No flips happening   â”‚ â€¢ Lower threshold      â”‚
â”‚                      â”‚ â€¢ Check predictions    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Too many flips       â”‚ â€¢ Raise threshold      â”‚
â”‚                      â”‚ â€¢ Verify imbalance     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Worse performance    â”‚ â€¢ Check stage 1 model  â”‚
â”‚                      â”‚ â€¢ Tune threshold       â”‚
â”‚                      â”‚ â€¢ Try val set tuning   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Import errors        â”‚ â€¢ Check requirements   â”‚
â”‚                      â”‚ â€¢ Verify file paths    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Mathematical Formulas

### Z-score Normalization
```
Z = (Î·Ì‚ - Î¼) / Ïƒ

where:
  Î·Ì‚ = prediction for current sample
  Î¼ = mean prediction on majority class
  Ïƒ = std prediction on majority class
```

### Flip Rate Function
```
Ï = max(tanh(Z - t_flip), 0)

Properties:
  â€¢ Ï âˆˆ [0, 1]
  â€¢ Ï = 0 when Z â‰¤ t_flip
  â€¢ Ï â†’ 1 as Z â†’ âˆ
  â€¢ Smooth transition at threshold
```

### Bernoulli Sampling
```
U ~ Bernoulli(Ï)

P(U = 1) = Ï
P(U = 0) = 1 - Ï

Flip label if U = 1
```

---

## Visualization Legend

```
When you see plots:

Colors:
  ğŸ”µ Blue   = Majority samples (kept)
  ğŸ”´ Red    = Majority samples (flipped)
  ğŸŸ¢ Green  = Minority samples (original)
  âš« Black  = Thresholds/means

Lines:
  â”â”â” Solid     = Function/curve
  â•Œâ•Œâ•Œ Dashed    = Threshold/reference
```

---

## Key Insights

```
ğŸ’¡ High Z-score = Outlier in majority class
   â†’ Sample's prediction much higher than average
   â†’ Likely similar to minority class
   â†’ Good candidate for flipping

ğŸ’¡ Tanh function = Smooth transition
   â†’ Not a hard cutoff
   â†’ Gradual increase in flip probability
   â†’ Mathematically elegant

ğŸ’¡ Stochastic flipping = Regularization
   â†’ Not all high-Z samples flipped
   â†’ Adds randomness
   â†’ Different flips each run

ğŸ’¡ Minimal intervention = Data efficiency
   â†’ Only ~2% of samples flipped
   â†’ Preserves most original labels
   â†’ Uses real samples (not synthetic)
```

---

## Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Imbalanced   â”‚
â”‚   Dataset    â”‚
â”‚ (90% / 10%)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1:    â”‚
â”‚  Pre-train   â”‚ â† Standard training
â”‚   Classifier â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apply LNR    â”‚
â”‚ Algorithm 1  â”‚ â† Compute flips
â”‚ (noise model)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2:    â”‚
â”‚  Fine-tune   â”‚ â† Train with flips
â”‚ with Flipped â”‚
â”‚    Labels    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Improved    â”‚
â”‚   Balanced   â”‚ â† Better minority
â”‚  Classifier  â”‚   class accuracy
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Requirements

```bash
# Core
torch
torchvision
numpy

# Optional (for demo/visualization)
matplotlib
scikit-learn
```

Install:
```bash
pip install torch torchvision numpy matplotlib scikit-learn
```

---

## Citation

```bibtex
@inproceedings{hu2025lnr,
  title={Learning Imbalanced Data with Beneficial Label Noise},
  author={Hu, Guangzheng and Liu, Feng and Gong, Mingming and 
          Wang, Guanghui and Peng, Liuhua},
  booktitle={ICML},
  year={2025}
}
```

---

## Quick Help

```
For help, refer to:

ğŸ“– Full docs:      BINARY_README.md
ğŸ” Repo analysis:  REPO_ANALYSIS.md
ğŸ¯ This guide:     QUICK_REFERENCE.md
ğŸ’» Demo code:      demo_binary_lnr.py
ğŸš€ Train code:     train_binary_lnr.py
âš™ï¸  Core code:      binary_lnr.py
```

---

**Remember:** Start with `python demo_binary_lnr.py` to understand the algorithm!
